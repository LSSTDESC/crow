{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa52183-9d9b-4665-a114-482ded39f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a72f52a-f640-4fc5-aaf9-a398edc81a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/sps/lsst/users/ebarroso/crow\")\n",
    "from crow.cluster_modules.shear_profile import *\n",
    "from crow.recipes.binned_exact import ExactBinnedClusterRecipe\n",
    "from crow.recipes.binned_grid import GridBinnedClusterRecipe\n",
    "from crow.cluster_modules.mass_proxy import MurataUnbinned, MurataBinned\n",
    "from crow.cluster_modules.kernel import SpectroscopicRedshift\n",
    "from crow.cluster_modules.completeness_models import CompletenessAguena16\n",
    "from crow.cluster_modules.purity_models import PurityAguena16\n",
    "\n",
    "#from firecrown.models.cluster import ClusterProperty\n",
    "from crow.properties import ClusterProperty\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from scipy.integrate import dblquad, tplquad, simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09177aa0-dd06-4bf4-b52d-c42b63e99320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyccl as ccl\n",
    "hmf = ccl.halos.MassFuncTinker08(mass_def=\"200c\")\n",
    "cosmo = ccl.Cosmology(\n",
    "    Omega_c=0.2607,      # Cold dark matter density\n",
    "    Omega_b=0.04897,     # Baryon density\n",
    "    h=0.6766,            # Hubble parameter\n",
    "    sigma8=0.8102,       # Matter fluctuation amplitude\n",
    "    n_s=0.9665,          # Spectral index\n",
    ")\n",
    "cl_delta_sigma = ClusterShearProfile(cosmo, hmf, 4.0, True)\n",
    "#cl_delta_sigma.vectorized= True\n",
    "pivot_mass, pivot_redshift = 14.625862906, 0.6\n",
    "comp_dist = CompletenessAguena16()\n",
    "pur_dist = PurityAguena16()\n",
    "mass_distribution = MurataUnbinned(pivot_mass, pivot_redshift)\n",
    "mass_distribution_binned = MurataBinned(pivot_mass, pivot_redshift)\n",
    "redshift_distribution = SpectroscopicRedshift()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b708fef-7cdb-4239-84af-3f1520364076",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Parameters to be used in both recipes #####\n",
    "mass_grid_size = 30\n",
    "redshift_grid_size = 10\n",
    "proxy_grid_size = 12\n",
    "sky_area = 440\n",
    "mass_interval = (12.5, 15.0)\n",
    "cluster_theory = cl_delta_sigma\n",
    "z_bin = (0.2, 0.4)\n",
    "z_points = np.linspace(z_bin[0], z_bin[1], redshift_grid_size) \n",
    "proxy_bin = (1.0, 1.3)\n",
    "proxy_points = np.linspace(proxy_bin[0], proxy_bin[1], proxy_grid_size)\n",
    "radius_center = np.array([4.0])\n",
    "#################################################\n",
    "\n",
    "recipe_integral = ExactBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_binned,\n",
    "        completeness=comp_dist,\n",
    "    )\n",
    "\n",
    "recipe_grid = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        completeness=comp_dist,\n",
    "    proxy_grid_size=proxy_grid_size,\n",
    "    redshift_grid_size=redshift_grid_size,\n",
    "    mass_grid_size=mass_grid_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c7b083-c3ca-4f75-9726-02777bb5bfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00997866]\n"
     ]
    }
   ],
   "source": [
    "print(recipe_integral._completeness_distribution(np.array([mass_interval[0]]),np.array([z_bin[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f97ac-bcc5-4349-9bff-725fff9c068d",
   "metadata": {},
   "source": [
    "## Testing mass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61286b9-ed09-4354-8311-f60b17e62d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 87221.31147840535, dblquad integral 87220.6489541402\n",
      "Abs error 0.6625242651498411, rel error 7.595956612371779e-06\n"
     ]
    }
   ],
   "source": [
    "hmf_grid = recipe_grid._get_hmf_grid(z_points, sky_area, (z_points[0], z_points[-1]))\n",
    "log_mass_grid = recipe_grid.log_mass_grid\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    \"\"\"\n",
    "    Inputs from dblquad are SCALARS:\n",
    "    1. log_mass_scalar (y-variable in dblquad)\n",
    "    2. z_scalar (x-variable in dblquad)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convert scalars to single-element arrays to satisfy the methods\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "\n",
    "    # 2. Call the methods with the correct, named arrays\n",
    "    # comoving_volume(z, sky_area)\n",
    "    vol_array = recipe_integral.cluster_theory.comoving_volume(z_array, sky_area)\n",
    "    \n",
    "    # mass_function(log_mass, z)\n",
    "    hmf_array = recipe_integral.cluster_theory.mass_function(log_mass_array, z_array)\n",
    "    \n",
    "    # 3. Return the result as a scalar float\n",
    "    return (vol_array * hmf_array)[0]\n",
    "\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_mass = simpson(\n",
    "    y=hmf_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ac7fe-1355-4c01-aa19-c92a322b57c0",
   "metadata": {},
   "source": [
    "## Testing mass-richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2959bb4-3215-4d63-8c05-372f1fa0c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 0.07492115089000648, dblquad integral 0.0749396525986431\n",
      "Abs error 1.8501708636625702e-05, rel error 0.0002468881025605718\n"
     ]
    }
   ],
   "source": [
    "mass_richness_grid = recipe_grid._get_mass_richness_grid(z_points, proxy_points, None)\n",
    "\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    \"\"\"\n",
    "    Inputs from dblquad are SCALARS:\n",
    "    1. log_mass_scalar (y-variable in dblquad)\n",
    "    2. z_scalar (x-variable in dblquad)\n",
    "    \"\"\"\n",
    "    \n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "    return mass_distribution_binned.distribution(log_mass_array, z_array, proxy_bin)    \n",
    "\n",
    "z_grid_mesh, log_mass_grid_mesh = np.meshgrid(\n",
    "            z_points, log_mass_grid, indexing='ij'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "integral_mass_richness, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "\n",
    "integral_over_mass = simpson(\n",
    "    y=mass_richness_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=2\n",
    ")\n",
    "\n",
    "integral_over_proxy = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=proxy_points * np.log(10.0), \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "simpson_mass_richness = simpson(\n",
    "    y=integral_over_proxy, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_mass_richness}, dblquad integral {integral_mass_richness}\")\n",
    "print(f\"Abs error {abs(integral_mass_richness - simpson_mass_richness)}, rel error {abs(1.0 - simpson_mass_richness/integral_mass_richness)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd234884-7e95-4e76-bcd8-d75b11d67295",
   "metadata": {},
   "source": [
    "## Testing completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c1d58c-4397-435a-a5d1-686142142e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 0.28510822270960434, dblquad integral 0.2851081640654921\n",
      "Abs error 5.864411223299726e-08, rel error 2.0569075043219698e-07\n"
     ]
    }
   ],
   "source": [
    "comp_grid = recipe_grid._get_completeness_grid(z_points, None)\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "    return recipe_integral._completeness_distribution(log_mass_array, z_array)\n",
    "\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_mass = simpson(\n",
    "    y=comp_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb769e6-b65f-4dd8-b553-e05b81a2402b",
   "metadata": {},
   "source": [
    "## Testing Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccedb072-9661-49ab-ab3a-fe84e0db8700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " (12, 10)\n",
      "Simpson Integral 0.13497569756262148, dblquad integral 0.1349756810357927\n",
      "Abs error 1.6526828794383164e-08, rel error 1.224430110635666e-07\n"
     ]
    }
   ],
   "source": [
    "######################## Recipe with purity ########################\n",
    "#We do not use purity to test everywhere because the exact has no implementation yet#\n",
    "recipe_grid_impure = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        completeness=comp_dist,\n",
    "    purity = pur_dist,\n",
    "    proxy_grid_size=proxy_grid_size,\n",
    "    redshift_grid_size=redshift_grid_size,\n",
    "    mass_grid_size=mass_grid_size,\n",
    "    )\n",
    "########################\n",
    "pur_grid = recipe_grid_impure._get_purity_grid(z_points, proxy_points, None)\n",
    "print(f\"\\n\\n\",pur_grid.shape)\n",
    "\n",
    "def integrand(ln_proxy_scalar, z_scalar):\n",
    "    log_proxy_scalar = ln_proxy_scalar / np.log(10.0)\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_proxy_array = np.array([log_proxy_scalar])\n",
    "    return recipe_grid_impure._purity_distribution(z_array, log_proxy_array)\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: proxy_bin[0] * np.log(10.0), # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: proxy_bin[1] * np.log(10.0), # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_proxy = simpson(\n",
    "    y=pur_grid, \n",
    "    x=proxy_points * np.log(10.0), \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_proxy, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608587d5-2610-4a02-9005-73f2ae9fac53",
   "metadata": {},
   "source": [
    "## Testing Shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505b8dfa-d289-45d9-9ea7-1058b7d51e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral [3.10754723e+12], dblquad integral 3107478462523.7495\n",
      "Abs error [68772318.41601562], rel error [2.21312293e-05]\n"
     ]
    }
   ],
   "source": [
    "shear_grid = recipe_grid._get_shear_grid(z_points, np.array(radius_center), None)\n",
    "log_mass_grid = recipe_grid.log_mass_grid\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    \"\"\"\n",
    "    Inputs from dblquad are SCALARS:\n",
    "    1. log_mass_scalar (y-variable in dblquad)\n",
    "    2. z_scalar (x-variable in dblquad)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convert scalars to single-element arrays to satisfy the methods\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "    shear = recipe_integral.cluster_theory.compute_shear_profile(log_mass_array, z_array, radius_center = radius_center)\n",
    "    \n",
    "    # 3. Return the result as a scalar float\n",
    "    return shear[0]\n",
    "\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_mass = simpson(\n",
    "    y=shear_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n",
    "# print( recipe_integral.cluster_theory.compute_shear_profile(np.array([1.0,3.0])[:, None], np.array([1.0, 1.0, 2.0,1.0]), radius_center = np.array([2.0, 1.0,1.0])[:, None]))\n",
    "\n",
    "# print(np.array([1.0,1.0])[:, None])\n",
    "# print(np.array([2.0, 1.0,1.0])[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f04d1-09ec-46c7-acdb-688f6636b22a",
   "metadata": {},
   "source": [
    "## Testing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50694f5b-f501-4ab1-86f2-11c09b6faf9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.4) (1.0, 1.3)\n",
      "Simpson Integral 465.86054457432704, dblquad integral 465.88615450982513\n",
      "Abs error 0.025609935498096092, rel error 5.4970372590390504e-05\n",
      "First eval took: 0.0015347003936767578, second eval took: 0.0007061958312988281, integral took: 0.0427401065826416\n",
      "After reset: 0.0011837482452392578\n",
      " Counts 1, 2 ,3: (np.float64(465.86054457432704), np.float64(465.86054457432704), np.float64(465.86054457432704))\n"
     ]
    }
   ],
   "source": [
    "recipe_grid.setup()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area)\n",
    "t2 = time.time()\n",
    "print(z_bin, proxy_bin)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area)\n",
    "t4 = time.time()\n",
    "recipe_grid.setup()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9bca-43cd-4a2a-b85c-27f53082d40f",
   "metadata": {},
   "source": [
    "## Testing DeltaSigma Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbbafeee-635b-4770-8f08-96bfeffec78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.]\n",
      "[3.67451479e+15]\n",
      "Simpson Integral [3.67407585e+15], dblquad integral [3.67451479e+15]\n",
      "Abs error [4.38941366e+11], rel error [0.00011946]\n",
      "First eval took: 0.0026946067810058594, second eval took: 0.0008661746978759766, integral took: 0.11332583427429199\n",
      "After reset: 0.0024514198303222656\n",
      " Counts 1, 2 ,3: (array([3.67407585e+15]), array([3.67407585e+15]), array([3.67407585e+15]))\n"
     ]
    }
   ],
   "source": [
    "average_on = ClusterProperty.NONE\n",
    "average_on |= ClusterProperty.DELTASIGMA\n",
    "recipe_grid.setup()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center,sky_area=sky_area, average_on=average_on)\n",
    "t2 = time.time()\n",
    "print(radius_center)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "print(counts_integral)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t4 = time.time()\n",
    "recipe_grid.setup()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c38c6-b639-45ca-989e-33b1033fc1d7",
   "metadata": {},
   "source": [
    "## Testing Reduced Shear Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1d1b2e7-5286-4256-a06e-19d219db6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_reduced_shear = ClusterShearProfile(cosmo, hmf, 4.0, False, True)\n",
    "cl_reduced_shear.set_beta_parameters(10)\n",
    "cl_reduced_shear.set_beta_s_interp(1.1, 1.3)\n",
    "recipe_integral_reduced = ExactBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_reduced_shear,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_binned,\n",
    "        #completeness=comp_dist,\n",
    "    )\n",
    "\n",
    "recipe_grid_reduced = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_reduced_shear,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        #completeness=comp_dist,\n",
    "    proxy_grid_size=proxy_grid_size,\n",
    "    redshift_grid_size=redshift_grid_size,\n",
    "    mass_grid_size=mass_grid_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1475ea55-2c46-4874-b8d1-651f92eb6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral [1.22802065], dblquad integral [1.36087356]\n",
      "Abs error [0.13285291], rel error [0.09762325]\n",
      "First eval took: 0.04190349578857422, second eval took: 0.0007469654083251953, integral took: 0.28732752799987793\n",
      "After reset: 0.0007152557373046875\n",
      " Values 1, 2 ,3: (array([1.22802065]), array([1.22802065]), array([1.22802065]))\n"
     ]
    }
   ],
   "source": [
    "average_on = ClusterProperty.NONE\n",
    "average_on |= ClusterProperty.DELTASIGMA\n",
    "recipe_grid.setup()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid_reduced.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center,sky_area=sky_area, average_on=average_on)\n",
    "t2 = time.time()\n",
    "counts_integral = recipe_integral_reduced.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid_reduced.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t4 = time.time()\n",
    "recipe_grid.setup()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid_reduced.evaluate_theory_prediction_lensing_profile(z_edges=z_bin, log_proxy_edges=proxy_bin, radius_centers=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Values 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0614340-8f43-4971-bf58-ba8268f4bff2",
   "metadata": {},
   "source": [
    "## Testing Mean Log Mass Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19680b81-53a3-4a3c-b0cc-af012e62b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.4) (1.0, 1.3)\n",
      "Simpson Integral 6643.852449550316, dblquad integral 6644.228454470479\n",
      "Abs error 0.3760049201628135, rel error 5.659120885737057e-05\n",
      "First eval took: 0.0013675689697265625, second eval took: 0.0006885528564453125, integral took: 0.0020668506622314453\n",
      "After reset: 0.0012717247009277344\n",
      " Counts 1, 2 ,3: (np.float64(6643.852449550316), np.float64(6643.852449550316), np.float64(6643.852449550316))\n"
     ]
    }
   ],
   "source": [
    "average_on_mass = ClusterProperty.NONE\n",
    "average_on_mass |= ClusterProperty.MASS\n",
    "\n",
    "recipe_grid.setup()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t2 = time.time()\n",
    "print(z_bin, proxy_bin)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t4 = time.time()\n",
    "recipe_grid.setup()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb32809-467e-4eec-ae04-d58c6b52a284",
   "metadata": {},
   "source": [
    "## Testing Mean redshift Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48013b86-2d4a-49bc-94e8-a04f03e03c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.4) (1.0, 1.3)\n",
      "Simpson Integral 145.5788864174151, dblquad integral 145.5957901462481\n",
      "Abs error 0.016903728833000287, rel error 0.00011610039559539764\n",
      "First eval took: 0.0013070106506347656, second eval took: 0.0006926059722900391, integral took: 0.0016515254974365234\n",
      "After reset: 0.0012030601501464844\n",
      " Counts 1, 2 ,3: (np.float64(145.5788864174151), np.float64(145.5788864174151), np.float64(145.5788864174151))\n"
     ]
    }
   ],
   "source": [
    "average_on_mass = ClusterProperty.NONE\n",
    "average_on_mass |= ClusterProperty.REDSHIFT\n",
    "\n",
    "recipe_grid.setup()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t2 = time.time()\n",
    "print(z_bin, proxy_bin)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t4 = time.time()\n",
    "recipe_grid.setup()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d2461-258a-4f22-a545-2a189a16e146",
   "metadata": {},
   "source": [
    "## Test speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67d082c4-49b9-4a47-bafd-aaa784940307",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Parameters to be used in both recipes #####\n",
    "mass_grid_size = 40\n",
    "redshift_grid_size = 10\n",
    "proxy_grid_size = 10\n",
    "sky_area = 440\n",
    "mass_interval = (12.0, 17.0)\n",
    "cluster_theory = cl_delta_sigma\n",
    "z_bin = (0.2, 0.4)\n",
    "z_points = np.linspace(z_bin[0], z_bin[1], redshift_grid_size) \n",
    "proxy_bin = (1.0, 1.3)\n",
    "proxy_points = np.linspace(proxy_bin[0], proxy_bin[1], proxy_grid_size)\n",
    "radius_center = np.array([4.0])\n",
    "average_on_counts = ClusterProperty.NONE\n",
    "average_on_shear = ClusterProperty.NONE\n",
    "average_on_shear |= ClusterProperty.DELTASIGMA\n",
    "#################################################\n",
    "\n",
    "recipe_integral_speed = ExactBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_binned,\n",
    "        #completeness=comp_dist,\n",
    "    )\n",
    "\n",
    "recipe_grid_speed = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        #completeness=comp_dist,\n",
    "    proxy_grid_size=proxy_grid_size,\n",
    "    redshift_grid_size=redshift_grid_size,\n",
    "    mass_grid_size=mass_grid_size,\n",
    "    )\n",
    "recipe_grid_speed.setup()\n",
    "counts_grid = recipe_grid_speed.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area,average_on_counts)\n",
    "counts_integral = recipe_integral_speed.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_counts)\n",
    "\n",
    "counts_grid = recipe_grid_speed.evaluate_theory_prediction_lensing_profile(z_bin, proxy_bin, radius_center,sky_area, average_on_shear)\n",
    "t2 = time.time()\n",
    "counts_integral = recipe_integral_speed.evaluate_theory_prediction_lensing_profile(z_bin, proxy_bin, radius_center, sky_area,average_on_shear)\n",
    "t21 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bab1ac-1e14-4b76-be56-6e98da5cd035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- One-shot counts ---\n",
      "Grid time:    0.0523 s\n",
      "Integral time: 0.0023 s\n",
      "\n",
      "--- One-shot shear ---\n",
      "Grid time:    0.0021 s\n",
      "Integral time: 0.4202 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Your parameters\n",
    "mass_grid_size = 40\n",
    "redshift_grid_size = 12\n",
    "proxy_grid_size = 12\n",
    "sky_area = 440\n",
    "mass_interval = (12.0, 17.0)\n",
    "z_bin = (0.2, 0.4)\n",
    "proxy_bin = (1.0, 1.3)\n",
    "radius_center = np.array([4.0])\n",
    "\n",
    "average_on_counts = ClusterProperty.NONE\n",
    "average_on_shear  = ClusterProperty.DELTASIGMA\n",
    "\n",
    "# Recipes\n",
    "recipe_integral_speed = ExactBinnedClusterRecipe(\n",
    "    mass_interval=mass_interval,\n",
    "    cluster_theory=cl_delta_sigma,\n",
    "    redshift_distribution=redshift_distribution,\n",
    "    mass_distribution=mass_distribution_binned,\n",
    "    completeness=comp_dist,\n",
    ")\n",
    "\n",
    "recipe_grid_speed = GridBinnedClusterRecipe(\n",
    "    mass_interval=mass_interval,\n",
    "    cluster_theory=cl_delta_sigma,\n",
    "    redshift_distribution=redshift_distribution,\n",
    "    mass_distribution=mass_distribution,\n",
    "    completeness=comp_dist,\n",
    "    proxy_grid_size=proxy_grid_size,\n",
    "    redshift_grid_size=redshift_grid_size,\n",
    "    mass_grid_size=mass_grid_size,\n",
    ")\n",
    "\n",
    "# ---- One-shot timing: counts ----\n",
    "recipe_grid_speed.setup()\n",
    "\n",
    "t0 = time.time()\n",
    "counts_grid = recipe_grid_speed.evaluate_theory_prediction_counts(\n",
    "    z_bin, proxy_bin, sky_area, average_on_counts\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "counts_integral = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "    z_bin, proxy_bin, sky_area, average_on_counts\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\n--- One-shot counts ---\")\n",
    "print(\"Grid time:    %.4f s\" % (t1 - t0))\n",
    "print(\"Integral time: %.4f s\" % (t2 - t1))\n",
    "\n",
    "\n",
    "# ---- One-shot timing: shear ----\n",
    "t0 = time.time()\n",
    "shear_grid = recipe_grid_speed.evaluate_theory_prediction_lensing_profile(\n",
    "    z_bin, proxy_bin, radius_center, sky_area, average_on_shear\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "shear_integral = recipe_integral_speed.evaluate_theory_prediction_lensing_profile(\n",
    "    z_bin, proxy_bin, radius_center, sky_area, average_on_shear\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\n--- One-shot shear ---\")\n",
    "print(\"Grid time:    %.4f s\" % (t1 - t0))\n",
    "print(\"Integral time: %.4f s\" % (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01bb54c3-0ea1-4376-a7df-aa336ff5344f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOTAL GRID TIME = 0.23 s\n",
      "TOTAL INTEGRAL TIME = 24.13 s\n",
      "\n",
      "--- RESULTS COMPARISON ---\n",
      "Total number of bins computed: 20\n",
      "RMS Relative Diff (Counts): 3.2608e-04\n",
      "RMS Relative Diff (Shear):  3.7853e-04\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "richness_bins = [(0.3,0.4), (0.4,0.6), (0.6,0.8), (0.8,1.0)]   # example\n",
    "proxy_bins    = [(1.0,1.1), (1.1,1.2), (1.2,1.3), (1.3,1.4), (1.4,1.5)]\n",
    "radii         = np.linspace(1.0, 10.0, 4)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Total GRID computation time\n",
    "# ----------------------------------------------\n",
    "recipe_grid_speed.setup()\n",
    "counts_grid_arr = []\n",
    "shear_grid_arr = []\n",
    "tg0 = time.time()\n",
    "for rbin in richness_bins:\n",
    "    for pbin in proxy_bins:\n",
    "        counts_grid_arr.append(recipe_grid_speed.evaluate_theory_prediction_counts(rbin, pbin, sky_area, average_on_counts))\n",
    "        shear_grid_arr.append(recipe_grid_speed.evaluate_theory_prediction_lensing_profile(rbin, pbin, radii, sky_area, average_on_shear))\n",
    "tg1 = time.time()\n",
    "print(\"\\nTOTAL GRID TIME = %.2f s\" % (tg1 - tg0))\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Total INTEGRAL computation time\n",
    "# ----------------------------------------------\n",
    "counts_int_arr = []\n",
    "shear_int_arr = []\n",
    "ti0 = time.time()\n",
    "for rbin in richness_bins:\n",
    "    for pbin in proxy_bins:\n",
    "        counts_int_arr.append(recipe_integral_speed.evaluate_theory_prediction_counts(rbin, pbin, sky_area, average_on_counts))\n",
    "        shear_int_arr.append(recipe_integral_speed.evaluate_theory_prediction_lensing_profile(rbin, pbin, radii, sky_area, average_on_shear))\n",
    "ti1 = time.time()\n",
    "print(\"TOTAL INTEGRAL TIME = %.2f s\" % (ti1 - ti0))\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Comparison of Results\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Convert lists to NumPy arrays for easy calculation\n",
    "counts_grid = np.array(counts_grid_arr).flatten()\n",
    "counts_int = np.array(counts_int_arr).flatten()\n",
    "shear_grid = np.array(shear_grid_arr).flatten()\n",
    "shear_int = np.array(shear_int_arr).flatten()\n",
    "\n",
    "counts_diff = np.where(\n",
    "    counts_int != 0.0, \n",
    "    np.abs(counts_grid - counts_int) / np.abs(counts_int), \n",
    "    0.0\n",
    ")\n",
    "rms_counts_diff = np.sqrt(np.mean(counts_diff**2))\n",
    "\n",
    "shear_diff = np.where(\n",
    "    shear_int != 0.0, \n",
    "    np.abs(shear_grid - shear_int) / np.abs(shear_int), \n",
    "    0.0\n",
    ")\n",
    "rms_shear_diff = np.sqrt(np.mean(shear_diff**2))\n",
    "\n",
    "# Final Comparison Printout\n",
    "print(\"\\n--- RESULTS COMPARISON ---\")\n",
    "print(\"Total number of bins computed: %d\" % len(counts_grid))\n",
    "print(\"RMS Relative Diff (Counts): %.4e\" % rms_counts_diff)\n",
    "print(\"RMS Relative Diff (Shear):  %.4e\" % rms_shear_diff)\n",
    "print(\"--------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80383885-0945-4e1c-bf2e-ed64072d9130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BEGIN ACCURACY SWEEP WITH COUNTS =====\n",
      "\n",
      "Test bin:\n",
      "  z = (0.200, 0.400)\n",
      "  proxy = (1.000, 1.300)\n",
      "  R = 4.000\n",
      "\n",
      "---- Sweep proxy_grid_size ----\n",
      "\n",
      "Trying proxy_grid_size = 2\n",
      "  shear diff  = 0.009079\n",
      "  counts diff = 0.122054\n",
      "\n",
      "Trying proxy_grid_size = 5\n",
      "  shear diff  = 0.000024\n",
      "  counts diff = 0.000089\n",
      "  → Achieved ≤1% accuracy at proxy_grid_size = 5\n",
      "\n",
      "---- Sweep redshift_grid_size ----\n",
      "\n",
      "Trying redshift_grid_size = 2\n",
      "  shear diff  = 0.029323\n",
      "  counts diff = 0.035479\n",
      "\n",
      "Trying redshift_grid_size = 5\n",
      "  shear diff  = 0.000047\n",
      "  counts diff = 0.000107\n",
      "  → Achieved ≤1% accuracy at redshift_grid_size = 5\n",
      "\n",
      "---- Sweep mass_grid_size ----\n",
      "\n",
      "Trying mass_grid_size = 5\n",
      "  shear diff  = 0.141789\n",
      "  counts diff = 0.229839\n",
      "\n",
      "Trying mass_grid_size = 10\n",
      "  shear diff  = 0.226844\n",
      "  counts diff = 0.168378\n",
      "\n",
      "Trying mass_grid_size = 20\n",
      "  shear diff  = 0.045500\n",
      "  counts diff = 0.008294\n",
      "\n",
      "Trying mass_grid_size = 40\n",
      "  shear diff  = 0.000041\n",
      "  counts diff = 0.000096\n",
      "\n",
      "Trying mass_grid_size = 60\n",
      "  shear diff  = 0.000007\n",
      "  counts diff = 0.000010\n",
      "\n",
      "Trying mass_grid_size = 80\n"
     ]
    }
   ],
   "source": [
    "def frac_diff(a, b):\n",
    "    return np.abs(a - b) / (np.abs(b) + 1e-12)\n",
    "\n",
    "\n",
    "print(\"\\n===== BEGIN ACCURACY SWEEP WITH COUNTS =====\\n\")\n",
    "\n",
    "# --- ranges to test ---\n",
    "proxy_sweep    = [2, 5, 10, 20, 30, 40, 60, 80, 120, 160]\n",
    "z_sweep        = [2, 5, 10, 20, 30, 40]\n",
    "mass_sweep     = [5, 10, 20, 40, 60, 80, 120]\n",
    "\n",
    "# --- fixed test bin ---\n",
    "z_lo, z_hi = z_bin\n",
    "proxy_lo, proxy_hi = proxy_bin\n",
    "R = np.array(radius_center)\n",
    "\n",
    "print(\"Test bin:\")\n",
    "print(\"  z = (%.3f, %.3f)\" % z_bin)\n",
    "print(\"  proxy = (%.3f, %.3f)\" % proxy_bin)\n",
    "print(\"  R = %.3f\" % R.item())\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1) PROXY SWEEP\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n---- Sweep proxy_grid_size ----\")\n",
    "\n",
    "for pts in proxy_sweep:\n",
    "    print(f\"\\nTrying proxy_grid_size = {pts}\")\n",
    "\n",
    "    recipe_grid_speed_pts = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_delta_sigma,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        completeness=comp_dist,\n",
    "        proxy_grid_size=pts,\n",
    "        redshift_grid_size=redshift_grid_size,\n",
    "        mass_grid_size=mass_grid_size,\n",
    "    )\n",
    "    recipe_grid_speed_pts.setup()\n",
    "    \n",
    "    # Shear evaluation\n",
    "    val_shear = recipe_grid_speed_pts.evaluate_theory_prediction_lensing_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    \n",
    "    # Counts evaluation\n",
    "    counts_val = recipe_grid_speed_pts.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    recipe_grid_speed_pts.setup()\n",
    "    \n",
    "    # Reference shear\n",
    "    ref_shear = recipe_integral_speed.evaluate_theory_prediction_lensing_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    \n",
    "    # Reference counts\n",
    "    ref_counts = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    diff_shear  = frac_diff(val_shear, ref_shear).item()\n",
    "    diff_counts = frac_diff(counts_val, ref_counts).item()\n",
    "\n",
    "    print(\"  shear diff  = %.6f\" % diff_shear)\n",
    "    print(\"  counts diff = %.6f\" % diff_counts)\n",
    "\n",
    "    if diff_shear < 0.01 and diff_counts < 0.01:\n",
    "        print(\"  → Achieved ≤1% accuracy at proxy_grid_size =\", pts)\n",
    "        break\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2) REDSHIFT SWEEP\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n---- Sweep redshift_grid_size ----\")\n",
    "\n",
    "for pts in z_sweep:\n",
    "    print(f\"\\nTrying redshift_grid_size = {pts}\")\n",
    "\n",
    "    recipe_grid_speed_pts = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_delta_sigma,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        completeness=comp_dist,\n",
    "        proxy_grid_size=proxy_grid_size,\n",
    "        redshift_grid_size=pts,\n",
    "        mass_grid_size=mass_grid_size,\n",
    "    )\n",
    "    recipe_grid_speed_pts.setup()\n",
    "    \n",
    "    val_shear = recipe_grid_speed_pts.evaluate_theory_prediction_lensing_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    counts_val = recipe_grid_speed_pts.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "\n",
    "    ref_shear = recipe_integral_speed.evaluate_theory_prediction_lensing_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    ref_counts = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    diff_shear  = frac_diff(val_shear, ref_shear).item()\n",
    "    diff_counts = frac_diff(counts_val, ref_counts).item()\n",
    "    \n",
    "    print(\"  shear diff  = %.6f\" % diff_shear)\n",
    "    print(\"  counts diff = %.6f\" % diff_counts)\n",
    "\n",
    "    if diff_shear < 0.01 and diff_counts < 0.01:\n",
    "        print(\"  → Achieved ≤1% accuracy at redshift_grid_size =\", pts)\n",
    "        break\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3) MASS SWEEP\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n---- Sweep mass_grid_size ----\")\n",
    "\n",
    "for pts in mass_sweep:\n",
    "    print(f\"\\nTrying mass_grid_size = {pts}\")\n",
    "\n",
    "    recipe_grid_speed_pts = GridBinnedClusterRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_delta_sigma,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        completeness=comp_dist,\n",
    "        proxy_grid_size=proxy_grid_size,\n",
    "        redshift_grid_size=redshift_grid_size,\n",
    "        mass_grid_size=pts,\n",
    "    )\n",
    "    recipe_grid_speed_pts.setup()\n",
    "    \n",
    "    val_shear = recipe_grid_speed_pts.evaluate_theory_prediction_lensing_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    counts_val = recipe_grid_speed_pts.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin,  sky_area\n",
    "    )\n",
    "\n",
    "    ref_shear = recipe_integral_speed.evaluate_theory_prediction_lensing_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    ref_counts = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    diff_shear  = frac_diff(val_shear, ref_shear).item()\n",
    "    diff_counts = frac_diff(counts_val, ref_counts).item()\n",
    "    \n",
    "    print(\"  shear diff  = %.6f\" % diff_shear)\n",
    "    print(\"  counts diff = %.6f\" % diff_counts)\n",
    "\n",
    "    # Optional early break if both ≤1%\n",
    "    # if diff_shear < 0.01 and diff_counts < 0.01:\n",
    "    #     print(\"  → Achieved ≤1% accuracy at mass_grid_size =\", pts)\n",
    "    #     break\n",
    "\n",
    "\n",
    "print(\"\\n===== END ACCURACY SWEEP WITH COUNTS =====\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (firecrown20.0)",
   "language": "python",
   "name": "firecrown_20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
