{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa52183-9d9b-4665-a114-482ded39f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a72f52a-f640-4fc5-aaf9-a398edc81a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/sps/lsst/users/ebarroso/crow\")\n",
    "from crow.cluster_modules.shear_profile import *\n",
    "from crow.recipes.murata_binned_spec_z_grid_real import MurataBinnedSpecZRecipeGrid\n",
    "from crow.recipes.murata_binned_spec_z import MurataBinnedSpecZRecipe\n",
    "from crow.cluster_modules.mass_proxy import MurataBinned\n",
    "from crow.cluster_modules.kernel import SpectroscopicRedshift\n",
    "from crow.cluster_modules.purity import PurityAguena16\n",
    "from crow.cluster_modules.completeness import CompletenessAguena16\n",
    "#from firecrown.models.cluster import ClusterProperty\n",
    "from crow.properties import ClusterProperty\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from scipy.integrate import dblquad, tplquad, simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09177aa0-dd06-4bf4-b52d-c42b63e99320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyccl as ccl\n",
    "hmf = ccl.halos.MassFuncTinker08(mass_def=\"200c\")\n",
    "cosmo = ccl.Cosmology(\n",
    "    Omega_c=0.2607,      # Cold dark matter density\n",
    "    Omega_b=0.04897,     # Baryon density\n",
    "    h=0.6766,            # Hubble parameter\n",
    "    sigma8=0.8102,       # Matter fluctuation amplitude\n",
    "    n_s=0.9665,          # Spectral index\n",
    ")\n",
    "cl_delta_sigma = ClusterShearProfile(cosmo, hmf, 4.0, True)\n",
    "#cl_delta_sigma.vectorized= True\n",
    "pivot_mass, pivot_redshift = 14.625862906, 0.6\n",
    "comp_dist = CompletenessAguena16()\n",
    "pur_dist = PurityAguena16()\n",
    "mass_distribution = MurataBinned(pivot_mass, pivot_redshift)#, pur_dist)\n",
    "redshift_distribution = SpectroscopicRedshift()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b708fef-7cdb-4239-84af-3f1520364076",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Parameters to be used in both recipes #####\n",
    "mass_points = 30\n",
    "redshift_points = 10\n",
    "log_proxy_points = 10\n",
    "sky_area = 440\n",
    "mass_interval = (12.5, 15.0)\n",
    "cluster_theory = cl_delta_sigma\n",
    "z_bin = (0.2, 0.4)\n",
    "z_points = np.linspace(z_bin[0], z_bin[1], redshift_points) \n",
    "proxy_bin = (1.0, 1.3)\n",
    "proxy_points = np.linspace(proxy_bin[0], proxy_bin[1], log_proxy_points)\n",
    "radius_center = 4.0\n",
    "#################################################\n",
    "\n",
    "recipe_integral = MurataBinnedSpecZRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        #completeness=comp_dist,\n",
    "    )\n",
    "\n",
    "recipe_grid = MurataBinnedSpecZRecipeGrid(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        #completeness=comp_dist,\n",
    "    log_proxy_points=log_proxy_points,\n",
    "    redshift_points=redshift_points,\n",
    "    log_mass_points=mass_points,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c7b083-c3ca-4f75-9726-02777bb5bfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(recipe_integral.completeness_distribution(np.array([mass_interval[0]]),np.array([z_bin[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f97ac-bcc5-4349-9bff-725fff9c068d",
   "metadata": {},
   "source": [
    "## Testing mass function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61286b9-ed09-4354-8311-f60b17e62d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 87221.31147840535, dblquad integral 87220.6489541402\n",
      "Abs error 0.6625242651498411, rel error 7.595956612371779e-06\n"
     ]
    }
   ],
   "source": [
    "hmf_grid = recipe_grid.get_hmf_grid(z_points, sky_area, (z_points[0], z_points[-1]))\n",
    "log_mass_grid = recipe_grid.log_mass_grid\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    \"\"\"\n",
    "    Inputs from dblquad are SCALARS:\n",
    "    1. log_mass_scalar (y-variable in dblquad)\n",
    "    2. z_scalar (x-variable in dblquad)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convert scalars to single-element arrays to satisfy the methods\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "\n",
    "    # 2. Call the methods with the correct, named arrays\n",
    "    # comoving_volume(z, sky_area)\n",
    "    vol_array = recipe_integral.cluster_theory.comoving_volume(z_array, sky_area)\n",
    "    \n",
    "    # mass_function(log_mass, z)\n",
    "    hmf_array = recipe_integral.cluster_theory.mass_function(log_mass_array, z_array)\n",
    "    \n",
    "    # 3. Return the result as a scalar float\n",
    "    return (vol_array * hmf_array)[0]\n",
    "\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_mass = simpson(\n",
    "    y=hmf_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ac7fe-1355-4c01-aa19-c92a322b57c0",
   "metadata": {},
   "source": [
    "## Testing mass-richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2959bb4-3215-4d63-8c05-372f1fa0c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BECAUSE MASS RICHNESS INTEGRAL HAS PURITY INSIDE, INSTANTIATE A NEW ONE FOR THIS TEST\n",
      "Simpson Integral 0.07492096465752202, dblquad integral 0.0749396525986431\n",
      "Abs error 1.868794112108718e-05, rel error 0.0002493732019438477\n"
     ]
    }
   ],
   "source": [
    "mass_richness_grid = recipe_grid.get_mass_richness_grid(z_points, proxy_points, None)\n",
    "###################3\n",
    "print(f\"BECAUSE MASS RICHNESS INTEGRAL HAS PURITY INSIDE, INSTANTIATE A NEW ONE FOR THIS TEST\")\n",
    "mass_distribution_pure = MurataBinned(pivot_mass, pivot_redshift)\n",
    "recipe_integral_pure = MurataBinnedSpecZRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_pure,\n",
    "        completeness=comp_dist,\n",
    "    )\n",
    "################################\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    \"\"\"\n",
    "    Inputs from dblquad are SCALARS:\n",
    "    1. log_mass_scalar (y-variable in dblquad)\n",
    "    2. z_scalar (x-variable in dblquad)\n",
    "    \"\"\"\n",
    "    \n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "    return recipe_integral.mass_distribution.distribution(log_mass_array, z_array, proxy_bin)    \n",
    "\n",
    "z_grid_mesh, log_mass_grid_mesh = np.meshgrid(\n",
    "            z_points, log_mass_grid, indexing='ij'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "integral_mass_richness, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "\n",
    "integral_over_mass = simpson(\n",
    "    y=mass_richness_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=2\n",
    ")\n",
    "\n",
    "integral_over_proxy = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=proxy_points * np.log(10.0), \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "simpson_mass_richness = simpson(\n",
    "    y=integral_over_proxy, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_mass_richness}, dblquad integral {integral_mass_richness}\")\n",
    "print(f\"Abs error {abs(integral_mass_richness - simpson_mass_richness)}, rel error {abs(1.0 - simpson_mass_richness/integral_mass_richness)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd234884-7e95-4e76-bcd8-d75b11d67295",
   "metadata": {},
   "source": [
    "## Testing completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c1d58c-4397-435a-a5d1-686142142e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 0.5, dblquad integral 0.5\n",
      "Abs error 0.0, rel error 0.0\n"
     ]
    }
   ],
   "source": [
    "comp_grid = recipe_grid.get_completeness_grid(z_points, None)\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "    return recipe_integral.completeness_distribution(log_mass_array, z_array)\n",
    "\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_mass = simpson(\n",
    "    y=comp_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb769e6-b65f-4dd8-b553-e05b81a2402b",
   "metadata": {},
   "source": [
    "## Testing Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccedb072-9661-49ab-ab3a-fe84e0db8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pur_grid = recipe_grid.compute_purity_grid(z_points, proxy_points)\n",
    "# def integrand(ln_proxy_scalar, z_scalar):\n",
    "#     log_proxy_scalar = ln_proxy_scalar / np.log(10.0)\n",
    "#     z_array = np.array([z_scalar])\n",
    "#     log_proxy_array = np.array([log_proxy_scalar])\n",
    "#     return recipe_integral.mass_distribution.purity.distribution(z_array, log_proxy_array)\n",
    "\n",
    "\n",
    "\n",
    "# integral_hmf, error_estimate = dblquad(\n",
    "#     func=integrand, \n",
    "#     a=z_bin[0],       # lower limit for z (x-axis)\n",
    "#     b=z_bin[1],       # upper limit for z (x-axis)\n",
    "#     gfun=lambda x: proxy_bin[0]  * np.log(10.0), # lower limit for log_mass (y-axis)\n",
    "#     hfun=lambda x: proxy_bin[1] * np.log(10.0), # upper limit for log_mass (y-axis)\n",
    "# )\n",
    "# integral_over_proxy = simpson(\n",
    "#     y=pur_grid, \n",
    "#     x=proxy_points, \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# # The result is the final scalar integrated number count\n",
    "# simpson_hmf = simpson(\n",
    "#     y=integral_over_proxy * np.log(10.0), \n",
    "#     x=z_points, \n",
    "#     axis=0\n",
    "# )\n",
    "\n",
    "# print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "# print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608587d5-2610-4a02-9005-73f2ae9fac53",
   "metadata": {},
   "source": [
    "## Testing Shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505b8dfa-d289-45d9-9ea7-1058b7d51e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 3107547234842.1655, dblquad integral 3107478462523.7495\n",
      "Abs error 68772318.41601562, rel error 2.2131229305610844e-05\n"
     ]
    }
   ],
   "source": [
    "shear_grid = recipe_grid.get_shear_grid(z_points, radius_center, None)\n",
    "log_mass_grid = recipe_grid.log_mass_grid\n",
    "def integrand(log_mass_scalar, z_scalar):\n",
    "    \"\"\"\n",
    "    Inputs from dblquad are SCALARS:\n",
    "    1. log_mass_scalar (y-variable in dblquad)\n",
    "    2. z_scalar (x-variable in dblquad)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Convert scalars to single-element arrays to satisfy the methods\n",
    "    z_array = np.array([z_scalar])\n",
    "    log_mass_array = np.array([log_mass_scalar])\n",
    "    shear = recipe_integral.cluster_theory.compute_shear_profile(log_mass_array, z_array, radius_center = radius_center)\n",
    "    \n",
    "    # 3. Return the result as a scalar float\n",
    "    return shear[0]\n",
    "\n",
    "\n",
    "\n",
    "integral_hmf, error_estimate = dblquad(\n",
    "    func=integrand, \n",
    "    a=z_bin[0],       # lower limit for z (x-axis)\n",
    "    b=z_bin[1],       # upper limit for z (x-axis)\n",
    "    gfun=lambda x: mass_interval[0], # lower limit for log_mass (y-axis)\n",
    "    hfun=lambda x: mass_interval[1]  # upper limit for log_mass (y-axis)\n",
    ")\n",
    "integral_over_mass = simpson(\n",
    "    y=shear_grid, \n",
    "    x=log_mass_grid, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step B: Integrate over the redshift dimension (axis=0 of the new array)\n",
    "# The result is the final scalar integrated number count\n",
    "simpson_hmf = simpson(\n",
    "    y=integral_over_mass, \n",
    "    x=z_points, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "print(f\"Simpson Integral {simpson_hmf}, dblquad integral {integral_hmf}\")\n",
    "print(f\"Abs error {abs(integral_hmf - simpson_hmf)}, rel error {abs(1.0 - simpson_hmf/integral_hmf)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f04d1-09ec-46c7-acdb-688f6636b22a",
   "metadata": {},
   "source": [
    "## Testing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50694f5b-f501-4ab1-86f2-11c09b6faf9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.4) (1.0, 1.3)\n",
      "Simpson Integral 492.44829165465876, dblquad integral 492.4758087098605\n",
      "Abs error 0.027517055201712992, rel error 5.587493784475761e-05\n",
      "First eval took: 0.0013136863708496094, second eval took: 0.0006847381591796875, integral took: 0.04209423065185547\n",
      "After reset: 0.0011701583862304688\n",
      " Counts 1, 2 ,3: (np.float64(492.44829165465876), np.float64(492.44829165465876), np.float64(492.44829165465876))\n"
     ]
    }
   ],
   "source": [
    "recipe_grid.reset_grids_cache()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area)\n",
    "t2 = time.time()\n",
    "print(z_bin, proxy_bin)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area)\n",
    "t4 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9bca-43cd-4a2a-b85c-27f53082d40f",
   "metadata": {},
   "source": [
    "## Testing DeltaSigma Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbbafeee-635b-4770-8f08-96bfeffec78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 3831544942570779.0, dblquad integral 3831984682268562.0\n",
      "Abs error 439739697783.0, rel error 0.00011475507713221145\n",
      "First eval took: 0.12030196189880371, second eval took: 0.0007197856903076172, integral took: 0.12692046165466309\n",
      "After reset: 0.11747241020202637\n",
      " Counts 1, 2 ,3: (np.float64(3831544942570779.0), np.float64(3831544942570779.0), np.float64(3831544942570779.0))\n"
     ]
    }
   ],
   "source": [
    "average_on = ClusterProperty.NONE\n",
    "average_on |= ClusterProperty.DELTASIGMA\n",
    "recipe_grid.reset_grids_cache()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center,sky_area=sky_area, average_on=average_on)\n",
    "t2 = time.time()\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t4 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c38c6-b639-45ca-989e-33b1033fc1d7",
   "metadata": {},
   "source": [
    "## Testing Reduced Shear Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1d1b2e7-5286-4256-a06e-19d219db6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_reduced_shear = ClusterShearProfile(cosmo, hmf, 4.0, False, True)\n",
    "cl_reduced_shear.set_beta_parameters(10)\n",
    "cl_reduced_shear.set_beta_s_interp(1.1, 1.3)\n",
    "\n",
    "recipe_integral_reduced = MurataBinnedSpecZRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_reduced_shear,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        #completeness=comp_dist,\n",
    "    )\n",
    "\n",
    "recipe_grid_reduced = MurataBinnedSpecZRecipeGrid(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_reduced_shear,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution,\n",
    "        #completeness=comp_dist,\n",
    "    log_proxy_points=log_proxy_points,\n",
    "    redshift_points=redshift_points,\n",
    "    log_mass_points=mass_points,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1475ea55-2c46-4874-b8d1-651f92eb6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson Integral 1.2280211540730646, dblquad integral 1.360873557658125\n",
      "Abs error 0.13285240358506045, rel error 0.09762288556306509\n",
      "First eval took: 0.2346971035003662, second eval took: 0.0007424354553222656, integral took: 0.31749939918518066\n",
      "After reset: 0.0006551742553710938\n",
      " Values 1, 2 ,3: (np.float64(1.2280211540730646), np.float64(1.2280211540730646), np.float64(1.2280211540730646))\n"
     ]
    }
   ],
   "source": [
    "average_on = ClusterProperty.NONE\n",
    "average_on |= ClusterProperty.DELTASIGMA\n",
    "recipe_grid.reset_grids_cache()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid_reduced.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center,sky_area=sky_area, average_on=average_on)\n",
    "t2 = time.time()\n",
    "counts_integral = recipe_integral_reduced.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid_reduced.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t4 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid_reduced.evaluate_theory_prediction_shear_profile(z_edges=z_bin, mass_proxy_edges=proxy_bin, radius_center=radius_center, sky_area=sky_area,average_on=average_on)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Values 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0614340-8f43-4971-bf58-ba8268f4bff2",
   "metadata": {},
   "source": [
    "## Testing Mean Log Mass Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19680b81-53a3-4a3c-b0cc-af012e62b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.4) (1.0, 1.3)\n",
      "Simpson Integral 7019.099215264838, dblquad integral 7019.501486555358\n",
      "Abs error 0.40227129051982047, rel error 5.7307672245765495e-05\n",
      "First eval took: 0.0013289451599121094, second eval took: 0.0007107257843017578, integral took: 0.0021064281463623047\n",
      "After reset: 0.0012080669403076172\n",
      " Counts 1, 2 ,3: (np.float64(7019.099215264838), np.float64(7019.099215264838), np.float64(7019.099215264838))\n"
     ]
    }
   ],
   "source": [
    "average_on_mass = ClusterProperty.NONE\n",
    "average_on_mass |= ClusterProperty.MASS\n",
    "\n",
    "recipe_grid.reset_grids_cache()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t2 = time.time()\n",
    "print(z_bin, proxy_bin)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t4 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb32809-467e-4eec-ae04-d58c6b52a284",
   "metadata": {},
   "source": [
    "## Testing Mean redshift Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48013b86-2d4a-49bc-94e8-a04f03e03c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.4) (1.0, 1.3)\n",
      "Simpson Integral 153.81441380278108, dblquad integral 153.83237337163465\n",
      "Abs error 0.017959568853569863, rel error 0.00011674765499580797\n",
      "First eval took: 0.0012233257293701172, second eval took: 0.0006618499755859375, integral took: 0.0015745162963867188\n",
      "After reset: 0.0011487007141113281\n",
      " Counts 1, 2 ,3: (np.float64(153.81441380278108), np.float64(153.81441380278108), np.float64(153.81441380278108))\n"
     ]
    }
   ],
   "source": [
    "average_on_mass = ClusterProperty.NONE\n",
    "average_on_mass |= ClusterProperty.REDSHIFT\n",
    "\n",
    "recipe_grid.reset_grids_cache()\n",
    "t1 = time.time()\n",
    "counts_grid = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t2 = time.time()\n",
    "print(z_bin, proxy_bin)\n",
    "counts_integral = recipe_integral.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t21 = time.time()\n",
    "t3 = time.time()\n",
    "counts_grid_2 = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t4 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "t5 = time.time()\n",
    "counts_grid_3 = recipe_grid.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area, average_on_mass)\n",
    "t6 = time.time()\n",
    "print(f\"Simpson Integral {counts_grid}, dblquad integral {counts_integral}\")\n",
    "print(f\"Abs error {abs(counts_integral - counts_grid)}, rel error {abs(1.0 - counts_grid/counts_integral)}\")\n",
    "print(f\"First eval took: {t2-t1}, second eval took: {t4-t3}, integral took: {t21-t2}\")\n",
    "print(f\"After reset: {t6-t5}\")\n",
    "print(f\" Counts 1, 2 ,3: {counts_grid, counts_grid_2, counts_grid_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d2461-258a-4f22-a545-2a189a16e146",
   "metadata": {},
   "source": [
    "## Test speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67d082c4-49b9-4a47-bafd-aaa784940307",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_distribution_pure = MurataBinned(pivot_mass, pivot_redshift)\n",
    "\n",
    "\n",
    "##### Parameters to be used in both recipes #####\n",
    "mass_points = 40\n",
    "redshift_points = 10\n",
    "log_proxy_points = 10\n",
    "sky_area = 440\n",
    "mass_interval = (12.0, 17.0)\n",
    "cluster_theory = cl_delta_sigma\n",
    "z_bin = (0.2, 0.4)\n",
    "z_points = np.linspace(z_bin[0], z_bin[1], redshift_points) \n",
    "proxy_bin = (1.0, 1.3)\n",
    "proxy_points = np.linspace(proxy_bin[0], proxy_bin[1], log_proxy_points)\n",
    "radius_center = 4.0\n",
    "average_on_counts = ClusterProperty.NONE\n",
    "average_on_shear = ClusterProperty.NONE\n",
    "average_on_shear |= ClusterProperty.DELTASIGMA\n",
    "#################################################\n",
    "\n",
    "recipe_integral_speed = MurataBinnedSpecZRecipe(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_pure,\n",
    "        #completeness=comp_dist,\n",
    "    )\n",
    "\n",
    "recipe_grid_speed = MurataBinnedSpecZRecipeGrid(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cluster_theory,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_pure,\n",
    "        #completeness=comp_dist,\n",
    "    log_proxy_points=log_proxy_points,\n",
    "    redshift_points=redshift_points,\n",
    "    log_mass_points=mass_points,\n",
    "    )\n",
    "recipe_grid_speed.reset_grids_cache()\n",
    "counts_grid = recipe_grid_speed.evaluate_theory_prediction_base(z_bin, proxy_bin, sky_area,average_on_counts)\n",
    "counts_integral = recipe_integral_speed.evaluate_theory_prediction_counts(z_bin, proxy_bin, sky_area, average_on_counts)\n",
    "\n",
    "counts_grid = recipe_grid_speed.evaluate_theory_prediction_shear_profile(z_bin, proxy_bin, radius_center,sky_area, average_on_shear)\n",
    "t2 = time.time()\n",
    "counts_integral = recipe_integral_speed.evaluate_theory_prediction_shear_profile(z_bin, proxy_bin, radius_center, sky_area,average_on_shear)\n",
    "t21 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bab1ac-1e14-4b76-be56-6e98da5cd035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- One-shot counts ---\n",
      "Grid time:    0.0508 s\n",
      "Integral time: 0.0022 s\n",
      "\n",
      "--- One-shot shear ---\n",
      "Grid time:    0.1892 s\n",
      "Integral time: 0.4594 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Your parameters\n",
    "mass_distribution_pure = MurataBinned(pivot_mass, pivot_redshift)\n",
    "\n",
    "mass_points = 40\n",
    "redshift_points = 12\n",
    "log_proxy_points = 12\n",
    "sky_area = 440\n",
    "mass_interval = (12.0, 17.0)\n",
    "z_bin = (0.2, 0.4)\n",
    "proxy_bin = (1.0, 1.3)\n",
    "radius_center = 4.0\n",
    "\n",
    "average_on_counts = ClusterProperty.NONE\n",
    "average_on_shear  = ClusterProperty.DELTASIGMA\n",
    "\n",
    "# Recipes\n",
    "recipe_integral_speed = MurataBinnedSpecZRecipe(\n",
    "    mass_interval=mass_interval,\n",
    "    cluster_theory=cl_delta_sigma,\n",
    "    redshift_distribution=redshift_distribution,\n",
    "    mass_distribution=mass_distribution_pure,\n",
    "    #completeness=comp_dist,\n",
    ")\n",
    "\n",
    "recipe_grid_speed = MurataBinnedSpecZRecipeGrid(\n",
    "    mass_interval=mass_interval,\n",
    "    cluster_theory=cl_delta_sigma,\n",
    "    redshift_distribution=redshift_distribution,\n",
    "    mass_distribution=mass_distribution_pure,\n",
    "    #completeness=comp_dist,\n",
    "    log_proxy_points=log_proxy_points,\n",
    "    redshift_points=redshift_points,\n",
    "    log_mass_points=mass_points,\n",
    ")\n",
    "\n",
    "# ---- One-shot timing: counts ----\n",
    "recipe_grid_speed.reset_grids_cache()\n",
    "\n",
    "t0 = time.time()\n",
    "counts_grid = recipe_grid_speed.evaluate_theory_prediction_base(\n",
    "    z_bin, proxy_bin, sky_area, average_on_counts\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "counts_integral = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "    z_bin, proxy_bin, sky_area, average_on_counts\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\n--- One-shot counts ---\")\n",
    "print(\"Grid time:    %.4f s\" % (t1 - t0))\n",
    "print(\"Integral time: %.4f s\" % (t2 - t1))\n",
    "\n",
    "\n",
    "# ---- One-shot timing: shear ----\n",
    "t0 = time.time()\n",
    "shear_grid = recipe_grid_speed.evaluate_theory_prediction_shear_profile(\n",
    "    z_bin, proxy_bin, radius_center, sky_area, average_on_shear\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "shear_integral = recipe_integral_speed.evaluate_theory_prediction_shear_profile(\n",
    "    z_bin, proxy_bin, radius_center, sky_area, average_on_shear\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\n--- One-shot shear ---\")\n",
    "print(\"Grid time:    %.4f s\" % (t1 - t0))\n",
    "print(\"Integral time: %.4f s\" % (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01bb54c3-0ea1-4376-a7df-aa336ff5344f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOTAL GRID TIME = 3.34 s\n",
      "TOTAL INTEGRAL TIME = 26.71 s\n",
      "\n",
      "--- RESULTS COMPARISON ---\n",
      "Total number of bins computed: 80\n",
      "RMS Relative Diff (Counts): 3.0806e-04\n",
      "RMS Relative Diff (Shear):  3.7307e-04\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "richness_bins = [(0.3,0.4), (0.4,0.6), (0.6,0.8), (0.8,1.0)]   # example\n",
    "proxy_bins    = [(1.0,1.1), (1.1,1.2), (1.2,1.3), (1.3,1.4), (1.4,1.5)]\n",
    "radii         = np.linspace(1.0, 10.0, 4)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Total GRID computation time\n",
    "# ----------------------------------------------\n",
    "recipe_grid_speed.reset_grids_cache()\n",
    "counts_grid_arr = []\n",
    "shear_grid_arr = []\n",
    "tg0 = time.time()\n",
    "for rbin in richness_bins:\n",
    "    for pbin in proxy_bins:\n",
    "        for R in radii:\n",
    "            counts_grid_arr.append(recipe_grid_speed.evaluate_theory_prediction_base(rbin, pbin, sky_area, average_on_counts))\n",
    "            shear_grid_arr.append(recipe_grid_speed.evaluate_theory_prediction_shear_profile(rbin, pbin, R, sky_area, average_on_shear))\n",
    "tg1 = time.time()\n",
    "print(\"\\nTOTAL GRID TIME = %.2f s\" % (tg1 - tg0))\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Total INTEGRAL computation time\n",
    "# ----------------------------------------------\n",
    "counts_int_arr = []\n",
    "shear_int_arr = []\n",
    "ti0 = time.time()\n",
    "for rbin in richness_bins:\n",
    "    for pbin in proxy_bins:\n",
    "        for R in radii:\n",
    "            counts_int_arr.append(recipe_integral_speed.evaluate_theory_prediction_counts(rbin, pbin, sky_area, average_on_counts))\n",
    "            shear_int_arr.append(recipe_integral_speed.evaluate_theory_prediction_shear_profile(rbin, pbin, R, sky_area, average_on_shear))\n",
    "ti1 = time.time()\n",
    "print(\"TOTAL INTEGRAL TIME = %.2f s\" % (ti1 - ti0))\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Comparison of Results\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Convert lists to NumPy arrays for easy calculation\n",
    "counts_grid = np.array(counts_grid_arr)\n",
    "counts_int = np.array(counts_int_arr)\n",
    "shear_grid = np.array(shear_grid_arr)\n",
    "shear_int = np.array(shear_int_arr)\n",
    "\n",
    "counts_diff = np.where(\n",
    "    counts_int != 0.0, \n",
    "    np.abs(counts_grid - counts_int) / np.abs(counts_int), \n",
    "    0.0\n",
    ")\n",
    "rms_counts_diff = np.sqrt(np.mean(counts_diff**2))\n",
    "\n",
    "shear_diff = np.where(\n",
    "    shear_int != 0.0, \n",
    "    np.abs(shear_grid - shear_int) / np.abs(shear_int), \n",
    "    0.0\n",
    ")\n",
    "rms_shear_diff = np.sqrt(np.mean(shear_diff**2))\n",
    "\n",
    "# Final Comparison Printout\n",
    "print(\"\\n--- RESULTS COMPARISON ---\")\n",
    "print(\"Total number of bins computed: %d\" % len(counts_grid))\n",
    "print(\"RMS Relative Diff (Counts): %.4e\" % rms_counts_diff)\n",
    "print(\"RMS Relative Diff (Shear):  %.4e\" % rms_shear_diff)\n",
    "print(\"--------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58905869-59a8-4f10-a321-9a4f6ffe87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterProperty.DELTASIGMA\n"
     ]
    }
   ],
   "source": [
    "print(average_on_shear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80383885-0945-4e1c-bf2e-ed64072d9130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BEGIN ACCURACY SWEEP WITH COUNTS =====\n",
      "\n",
      "Test bin:\n",
      "  z = (0.200, 0.400)\n",
      "  proxy = (1.000, 1.300)\n",
      "  R = 4.000\n",
      "\n",
      "---- Sweep log_proxy_points ----\n",
      "\n",
      "Trying log_proxy_points = 2\n",
      "  shear diff  = 0.024303\n",
      "  counts diff = 0.148385\n",
      "\n",
      "Trying log_proxy_points = 5\n",
      "  shear diff  = 0.000034\n",
      "  counts diff = 0.000131\n",
      "  → Achieved ≤1% accuracy at log_proxy_points = 5\n",
      "\n",
      "---- Sweep redshift_points ----\n",
      "\n",
      "Trying redshift_points = 2\n",
      "  shear diff  = 0.030061\n",
      "  counts diff = 0.036235\n",
      "\n",
      "Trying redshift_points = 5\n",
      "  shear diff  = 0.000044\n",
      "  counts diff = 0.000093\n",
      "  → Achieved ≤1% accuracy at redshift_points = 5\n",
      "\n",
      "---- Sweep log_mass_points ----\n",
      "\n",
      "Trying log_mass_points = 5\n",
      "  shear diff  = 0.109240\n",
      "  counts diff = 0.261848\n",
      "\n",
      "Trying log_mass_points = 10\n",
      "  shear diff  = 0.225185\n",
      "  counts diff = 0.173935\n",
      "\n",
      "Trying log_mass_points = 20\n",
      "  shear diff  = 0.041281\n",
      "  counts diff = 0.003686\n",
      "\n",
      "Trying log_mass_points = 40\n",
      "  shear diff  = 0.000037\n",
      "  counts diff = 0.000082\n",
      "\n",
      "Trying log_mass_points = 60\n",
      "  shear diff  = 0.000007\n",
      "  counts diff = 0.000010\n",
      "\n",
      "Trying log_mass_points = 80\n",
      "  shear diff  = 0.000007\n",
      "  counts diff = 0.000010\n",
      "\n",
      "Trying log_mass_points = 120\n",
      "  shear diff  = 0.000007\n",
      "  counts diff = 0.000010\n",
      "\n",
      "===== END ACCURACY SWEEP WITH COUNTS =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def frac_diff(a, b):\n",
    "    return np.abs(a - b) / (np.abs(b) + 1e-12)\n",
    "\n",
    "\n",
    "print(\"\\n===== BEGIN ACCURACY SWEEP WITH COUNTS =====\\n\")\n",
    "\n",
    "# --- ranges to test ---\n",
    "proxy_sweep    = [2, 5, 10, 20, 30, 40, 60, 80, 120, 160]\n",
    "z_sweep        = [2, 5, 10, 20, 30, 40]\n",
    "mass_sweep     = [5, 10, 20, 40, 60, 80, 120]\n",
    "\n",
    "# --- fixed test bin ---\n",
    "z_lo, z_hi = z_bin\n",
    "proxy_lo, proxy_hi = proxy_bin\n",
    "R = radius_center\n",
    "\n",
    "print(\"Test bin:\")\n",
    "print(\"  z = (%.3f, %.3f)\" % z_bin)\n",
    "print(\"  proxy = (%.3f, %.3f)\" % proxy_bin)\n",
    "print(\"  R = %.3f\" % R)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1) PROXY SWEEP\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n---- Sweep log_proxy_points ----\")\n",
    "\n",
    "for pts in proxy_sweep:\n",
    "    print(f\"\\nTrying log_proxy_points = {pts}\")\n",
    "\n",
    "    recipe_grid_speed_pts = MurataBinnedSpecZRecipeGrid(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_delta_sigma,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_pure,\n",
    "        #completeness=comp_dist,\n",
    "        log_proxy_points=pts,\n",
    "        redshift_points=redshift_points,\n",
    "        log_mass_points=mass_points,\n",
    "    )\n",
    "    recipe_grid_speed_pts.reset_grids_cache()\n",
    "    \n",
    "    # Shear evaluation\n",
    "    val_shear = recipe_grid_speed_pts.evaluate_theory_prediction_shear_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    \n",
    "    # Counts evaluation\n",
    "    counts_val = recipe_grid_speed_pts.evaluate_theory_prediction_base(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    recipe_grid_speed_pts.reset_grids_cache()\n",
    "    \n",
    "    # Reference shear\n",
    "    ref_shear = recipe_integral_speed.evaluate_theory_prediction_shear_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    \n",
    "    # Reference counts\n",
    "    ref_counts = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    diff_shear  = float(frac_diff(val_shear, ref_shear))\n",
    "    diff_counts = float(frac_diff(counts_val, ref_counts))\n",
    "\n",
    "    print(\"  shear diff  = %.6f\" % diff_shear)\n",
    "    print(\"  counts diff = %.6f\" % diff_counts)\n",
    "\n",
    "    if diff_shear < 0.01 and diff_counts < 0.01:\n",
    "        print(\"  → Achieved ≤1% accuracy at log_proxy_points =\", pts)\n",
    "        break\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2) REDSHIFT SWEEP\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n---- Sweep redshift_points ----\")\n",
    "\n",
    "for pts in z_sweep:\n",
    "    print(f\"\\nTrying redshift_points = {pts}\")\n",
    "\n",
    "    recipe_grid_speed_pts = MurataBinnedSpecZRecipeGrid(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_delta_sigma,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_pure,\n",
    "        #completeness=comp_dist,\n",
    "        log_proxy_points=log_proxy_points,\n",
    "        redshift_points=pts,\n",
    "        log_mass_points=mass_points,\n",
    "    )\n",
    "    recipe_grid_speed_pts.reset_grids_cache()\n",
    "    \n",
    "    val_shear = recipe_grid_speed_pts.evaluate_theory_prediction_shear_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    counts_val = recipe_grid_speed_pts.evaluate_theory_prediction_base(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "\n",
    "    ref_shear = recipe_integral_speed.evaluate_theory_prediction_shear_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    ref_counts = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    diff_shear  = float(frac_diff(val_shear, ref_shear))\n",
    "    diff_counts = float(frac_diff(counts_val, ref_counts))\n",
    "    \n",
    "    print(\"  shear diff  = %.6f\" % diff_shear)\n",
    "    print(\"  counts diff = %.6f\" % diff_counts)\n",
    "\n",
    "    if diff_shear < 0.01 and diff_counts < 0.01:\n",
    "        print(\"  → Achieved ≤1% accuracy at redshift_points =\", pts)\n",
    "        break\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3) MASS SWEEP\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n---- Sweep log_mass_points ----\")\n",
    "\n",
    "for pts in mass_sweep:\n",
    "    print(f\"\\nTrying log_mass_points = {pts}\")\n",
    "\n",
    "    recipe_grid_speed_pts = MurataBinnedSpecZRecipeGrid(\n",
    "        mass_interval=mass_interval,\n",
    "        cluster_theory=cl_delta_sigma,\n",
    "        redshift_distribution=redshift_distribution,\n",
    "        mass_distribution=mass_distribution_pure,\n",
    "        #completeness=comp_dist,\n",
    "        log_proxy_points=log_proxy_points,\n",
    "        redshift_points=redshift_points,\n",
    "        log_mass_points=pts,\n",
    "    )\n",
    "    recipe_grid_speed_pts.reset_grids_cache()\n",
    "    \n",
    "    val_shear = recipe_grid_speed_pts.evaluate_theory_prediction_shear_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    counts_val = recipe_grid_speed_pts.evaluate_theory_prediction_base(\n",
    "        z_bin, proxy_bin,  sky_area\n",
    "    )\n",
    "\n",
    "    ref_shear = recipe_integral_speed.evaluate_theory_prediction_shear_profile(\n",
    "        z_bin, proxy_bin, R, sky_area, average_on_shear\n",
    "    )\n",
    "    ref_counts = recipe_integral_speed.evaluate_theory_prediction_counts(\n",
    "        z_bin, proxy_bin, sky_area\n",
    "    )\n",
    "    \n",
    "    diff_shear  = float(frac_diff(val_shear, ref_shear))\n",
    "    diff_counts = float(frac_diff(counts_val, ref_counts))\n",
    "    \n",
    "    print(\"  shear diff  = %.6f\" % diff_shear)\n",
    "    print(\"  counts diff = %.6f\" % diff_counts)\n",
    "\n",
    "    # Optional early break if both ≤1%\n",
    "    # if diff_shear < 0.01 and diff_counts < 0.01:\n",
    "    #     print(\"  → Achieved ≤1% accuracy at log_mass_points =\", pts)\n",
    "    #     break\n",
    "\n",
    "\n",
    "print(\"\\n===== END ACCURACY SWEEP WITH COUNTS =====\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229e0a4-af20-4b0c-aa7b-c16ac133747c",
   "metadata": {},
   "source": [
    "## Compare vectorization times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cac3e65-7df4-416b-bd47-1c7cdd6b49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_grid = MurataBinnedSpecZRecipeGrid(\n",
    "    mass_interval=mass_interval,\n",
    "    cluster_theory=cl_delta_sigma,\n",
    "    redshift_distribution=redshift_distribution,\n",
    "    mass_distribution=mass_distribution,\n",
    "    completeness=comp_dist,\n",
    "    log_proxy_points=10,\n",
    "    redshift_points=11,\n",
    "    log_mass_points=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a982f68a-c761-4c8b-9f86-eeb7a922c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_bins = np.linspace(0.5, 1.5, 5)\n",
    "z_bins = np.linspace(0.2, 1.2, 11)\n",
    "r_vals = np.logspace(-1, 1,  10)\n",
    "\n",
    "average_on_counts = ClusterProperty.NONE\n",
    "average_on_shear = ClusterProperty.NONE\n",
    "average_on_shear |= ClusterProperty.DELTASIGMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16032b-3ad6-4412-9a44-e9eb50833a41",
   "metadata": {},
   "source": [
    "### One halo term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1403b64-f7e0-451b-876a-8459720cdd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old computation: 5.523 seconds\n",
      "vec computation: 0.058 seconds\n",
      "\n",
      "speed up factor   : 94.6\n",
      "max relative diff : 2.2e-16\n"
     ]
    }
   ],
   "source": [
    "# Old computation\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "t0 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_1h = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        for k, r in enumerate(r_vals):\n",
    "            val_shear_1h[i, j, k] = recipe_grid.evaluate_theory_prediction_shear_profile(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r, 440, average_on_shear\n",
    "            )\n",
    "time_old = time.time()-t0\n",
    "print(f\"old computation: {time_old:.3f} seconds\")\n",
    "\n",
    "# Vectorized computation\n",
    "\n",
    "t0 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_1h_vec = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        val_shear_1h_vec[i, j, :] = recipe_grid.evaluate_theory_prediction_shear_profile_vectorized(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r_vals, 440, average_on_shear\n",
    "            )\n",
    "time_vec = time.time()-t0\n",
    "print(f\"vec computation: {time_vec:.3f} seconds\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"speed up factor   : {time_old/time_vec:.1f}\")\n",
    "print(f\"max relative diff : {(val_shear_1h_vec/val_shear_1h-1).max():.2g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00804b52-ea7e-423e-bede-cd07d8fb0feb",
   "metadata": {},
   "source": [
    "### Two halo term\n",
    "\n",
    "The integration routine was changed, so first we make sure they are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b36dd660-fbd1-4b56-b25a-3867979f2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "221f9f21-e30a-45f4-bc94-e76df0524ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crow.cluster_modules._clmm_patches import (\n",
    "    _eval_2halo_term_generic_new,\n",
    "    _eval_2halo_term_generic_orig,\n",
    "    _eval_2halo_term_generic_vec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b629d45f-8526-443d-b3f4-da879286a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old computation: 21.758 seconds\n"
     ]
    }
   ],
   "source": [
    "# Original computation\n",
    "clmm.Modeling._eval_2halo_term_generic = _eval_2halo_term_generic_orig\n",
    "t0 = time.time()\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "recipe_grid.cluster_theory.two_halo_term = True\n",
    "recipe_grid.cluster_theory.boost_factor = False\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_orig = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        for k, r in enumerate(r_vals):\n",
    "            val_shear_2h_orig[i, j, k] = recipe_grid.evaluate_theory_prediction_shear_profile(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r, 440, average_on_shear\n",
    "            )\n",
    "time_old = time.time()-t0\n",
    "print(f\"old computation: {time_old:.3f} seconds\")\n",
    "\n",
    "\n",
    "excess_val_shear_2h_orig = val_shear_2h_orig-val_shear_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18ec00a3-2361-46a5-bd14-6545165300cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old computation: 21.595 seconds\n",
      "max relative diff : 0\n"
     ]
    }
   ],
   "source": [
    "clmm.Modeling._eval_2halo_term_generic = _eval_2halo_term_generic_new\n",
    "t0 = time.time()\n",
    "recipe_grid.cluster_theory.two_halo_term = True\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_new = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        for k, r in enumerate(r_vals):\n",
    "            val_shear_2h_new[i, j, k] = recipe_grid.evaluate_theory_prediction_shear_profile(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r, 440, average_on_shear\n",
    "            )\n",
    "time_old = time.time()-t0\n",
    "print(f\"old computation: {time_old:.3f} seconds\")\n",
    "\n",
    "excess_val_shear_2h_new = val_shear_2h_new-val_shear_1h\n",
    "print(f\"max relative diff : {(excess_val_shear_2h_new/excess_val_shear_2h_orig-1).max():.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "846b342b-9410-4edf-8319-19215caac508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old computation: 15.965 seconds\n",
      "max relative diff : 4.1e-07\n"
     ]
    }
   ],
   "source": [
    "clmm.Modeling._eval_2halo_term_generic = _eval_2halo_term_generic_vec\n",
    "t0 = time.time()\n",
    "recipe_grid.cluster_theory.two_halo_term = True\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_new2 = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        for k, r in enumerate(r_vals):\n",
    "            val_shear_2h_new2[i, j, k] = recipe_grid.evaluate_theory_prediction_shear_profile(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r, 440, average_on_shear\n",
    "            )\n",
    "time_old = time.time()-t0\n",
    "print(f\"old computation: {time_old:.3f} seconds\")\n",
    "\n",
    "excess_val_shear_2h_new2 = val_shear_2h_new2-val_shear_1h\n",
    "print(f\"max relative diff : {(excess_val_shear_2h_new2/excess_val_shear_2h_orig-1).max():.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6b99319-0146-4b76-9372-1f1ae03743e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec computation: 0.339 seconds\n",
      "max relative diff (new ) : 4.1e-07\n",
      "max relative diff (new2) : 4e-13\n",
      "max relative diff (new2 vs orig) : 4.1e-07\n"
     ]
    }
   ],
   "source": [
    "clmm.Modeling._eval_2halo_term_generic = _eval_2halo_term_generic_vec\n",
    "t0 = time.time()\n",
    "recipe_grid.cluster_theory.two_halo_term = True\n",
    "recipe_grid.cluster_theory.boost_factor = False\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_vec = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        val_shear_2h_vec[i, j, :] = recipe_grid.evaluate_theory_prediction_shear_profile_vectorized(\n",
    "            z_bins[j:j+2], proxy_bins[i:i+2], r_vals, 440, average_on_shear\n",
    "            )\n",
    "time_vec = time.time()-t0\n",
    "print(f\"vec computation: {time_vec:.3f} seconds\")\n",
    "\n",
    "excess_val_shear_2h_vec = val_shear_2h_vec-val_shear_1h_vec\n",
    "print(f\"max relative diff (new ) : {(excess_val_shear_2h_vec/excess_val_shear_2h_new-1).max():.2g}\")\n",
    "print(f\"max relative diff (new2) : {(excess_val_shear_2h_vec/excess_val_shear_2h_new2-1).max():.2g}\")\n",
    "print(f\"max relative diff (new2 vs orig) : {(excess_val_shear_2h_vec/excess_val_shear_2h_orig-1).max():.2g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba875f-3561-4840-b977-ab373418735a",
   "metadata": {},
   "source": [
    "### Boost factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f58ef1e9-1681-49f8-82de-ee0eea5c5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old computation: 16.120 seconds\n",
      "vec computation: 0.337 seconds\n",
      "\n",
      "speed up factor   : 47.8\n",
      "max relative diff : 4.4e-16\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "recipe_grid.cluster_theory.two_halo_term = True\n",
    "recipe_grid.cluster_theory.boost_factor = True\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_boost = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        for k, r in enumerate(r_vals):\n",
    "            val_shear_2h_boost[i, j, k] = recipe_grid.evaluate_theory_prediction_shear_profile(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r, 440, average_on_shear\n",
    "            )\n",
    "time_old = time.time()-t0\n",
    "print(f\"old computation: {time_old:.3f} seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_boost_vec = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        val_shear_2h_boost_vec[i, j, :] = recipe_grid.evaluate_theory_prediction_shear_profile_vectorized(\n",
    "            z_bins[j:j+2], proxy_bins[i:i+2], r_vals, 440, average_on_shear\n",
    "            )\n",
    "time_vec = time.time()-t0\n",
    "print(f\"vec computation: {time_vec:.3f} seconds\")\n",
    "\n",
    "print()\n",
    "print(f\"speed up factor   : {time_old/time_vec:.1f}\")\n",
    "print(f\"max relative diff : {(val_shear_2h_boost_vec/val_shear_2h_boost-1).max():.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c548dc06-43bf-47a7-84e9-0ad5414ad880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old computation: 5.500 seconds\n",
      "vec computation: 0.058 seconds\n",
      "\n",
      "speed up factor   : 94.2\n",
      "max relative diff : 2.2e-16\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "recipe_grid.cluster_theory.two_halo_term = False\n",
    "recipe_grid.cluster_theory.boost_factor = True\n",
    "recipe_grid.cluster_theory.vectorized = False\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_boost = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        for k, r in enumerate(r_vals):\n",
    "            val_shear_2h_boost[i, j, k] = recipe_grid.evaluate_theory_prediction_shear_profile(\n",
    "                z_bins[j:j+2], proxy_bins[i:i+2], r, 440, average_on_shear\n",
    "            )\n",
    "time_old = time.time()-t0\n",
    "print(f\"old computation: {time_old:.3f} seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "recipe_grid.reset_grids_cache()\n",
    "val_shear_2h_boost_vec = np.zeros(\n",
    "    (\n",
    "        proxy_bins.size-1,\n",
    "        z_bins.size-1,\n",
    "        r_vals.size,            \n",
    "    )\n",
    ")\n",
    "for i in range(proxy_bins.size-1):\n",
    "    for j in range(z_bins.size-1):\n",
    "        val_shear_2h_boost_vec[i, j, :] = recipe_grid.evaluate_theory_prediction_shear_profile_vectorized(\n",
    "            z_bins[j:j+2], proxy_bins[i:i+2], r_vals, 440, average_on_shear\n",
    "            )\n",
    "time_vec = time.time()-t0\n",
    "print(f\"vec computation: {time_vec:.3f} seconds\")\n",
    "\n",
    "print()\n",
    "print(f\"speed up factor   : {time_old/time_vec:.1f}\")\n",
    "print(f\"max relative diff : {(val_shear_2h_boost_vec/val_shear_2h_boost-1).max():.2g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (firecrown20.0)",
   "language": "python",
   "name": "firecrown_20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
